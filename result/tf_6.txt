running	8
hadoop	54
on	23
hpc	1
clusters
home
dataintensive	1
computing
hadoop
hadoop	1
hpc
this	1
guide	4
used	2
to	60
provide	2
instructions	2
based	1
myhadoop	16
01	1
(that	1
page	3
can	16
be	17
found	1
here	2
but	3
those	2
have	5
been	3
vastly	1
simplfied	1
with	14
the	109
release	1
of	39
02	2
this	23
is	30
a	35
placeholder	1
which	8
an	7
outofdate	1
user	4
now	2
supports	1
spark	1
and	38
2x	1
are	18
not	14
mentioned	2
if	4
you	21
interested	1
in	21
uptodate	1
version	8
please	1
contact	1
me	1
i	6
will	15
prioritize	1
updating	1
it

this	1
assumes	2
following

users	1
allowed	2
ssh	1
into	3
between	3
nodes	8
allocated	1
for	16
their	2
job	9
without	1
having	2
password
users	1
open	1
ports	1
>	1
1024	1
jobs'	1
nodes
each	1
node	8
has	6
local	4
scratch	4
space	4
that	21
shared	8
nodes†	1
users	1
write	1
it
for	1
sake	1
simplicity	1
we	1
also	6
assume	1
jobs	5
stresses	1
aspects	1
system	4
(like	3
io)	1
typically	1
considered	1
resource	6
allocation	1
so	2
nonhadoop	1
introduces	1
lot	1
unnecessary	1
complexity
we	1
installing	1
apache	2
1	7
as	7
opposed	2
2	4
critical	1
distinction	1
because	3
architecturally	1
(and	1
configurationally)	1
very	1
different	2
from	8
what	2
widespread	1
production	1
today	1
it	12
most	3
application	1
ecosystem	1
developed	2
future	2
incorporating	1
hadoop2yarn	1
support	2
it's	1
there	2
yet

the	1
exact	1
we're	1
using	6
less	1
important	1
am	1
latest	1
(as	2
writing)	1
121	3
although	2
no	3
reason	1
would	4
work	1
earlier	1
versions	1
we've	1
tested	1
104	1
111	1
ancient	1
020	1
releases

†	1
each	10
its	2
own	1
userwriteable	1
requirement	1
deploying	1
traditional	1
supercomputers	2
try	2
cover	1
more	1
advanced	1
topic	1
sharedparallel	1
filesystems	1
guide

basic	1
software	2
installation
install	2
030b
you	1
download	1
tarballs	2
following	4
locations

hadoop121bintargz
myhadoop
neither	1
these	3
requires	1
any	3
sort	1
proper	1
"installation"	1
simply	1
unpack	1
them	4
wherever	1
you'd	1
like	2
installed	2
eg

$	2
mkdir	2
~hadoopstack
$	2
cd	2
tar	2
zxvf	2
hadoop121bintargz
hadoop121
hadoop121eclipsetemplates
hadoop121eclipsetemplatesexternaltoolbuilders
hadoop121eclipsetemplateslaunches
hadoop121bin
hadoop121c++


$	1
myhadoop030btargz
myhadoop030b
myhadoop030bchangelog
myhadoop030blicense
myhadoop030breadmemd
myhadoop030bbin
myhadoop030bbinmyhadoopbootstrapsh

patch	1
configuration
myhadoop	1
ships	1
patch	2
file	7
myhadoop121patch	1
converts	1
default	2
configuration	8
files	5
ship	1
templates	3
then	7
copy	2
modify	1
when	4
wants	1
run	8
apply	1
patch

$	1
hadoop121conf
$	1
<	1
myhadoop030bmyhadoop121patch
patching	1
coresitexml
patching	1
hdfssitexml
patching	1
mapredsitexml
and	1
that's	1
your	22
cluster

starting	1
up	8
cluster
when	1
want	2
spin	2
cluster	13
need	2
do	4
within	1
environmenteither	1
noninteractive	1
script	8
or	3
interactive	1
job

step	1
define	4
$hadoop_home
define	1
$hadoop_home	5
previous	2
section's	1
example	3
$homehadoopstackhadoop121

$	1
export	6
hadoop_home=$homehadoopstackhadoop121
myhadoop	1
needs	1
know	1
where	3
$hadoop_homeconf	1
contains	3
patched	1
use	8
build	1
actual	1
configurations

adding	1
$hadoop_homebin	1
installation	3
directory	10
$path	1
just	1
matter	1
additional	1
convenience

$	1
path=$hadoop_homebin$path
$	1
path=$homehadoopstackmyhadoop030bbin$path
in	1
event	1
does	2
$java_home	1
properly	1
set	5
either	1
must	1
that

$	1
java_home=usrjavalatest
step	1
choose	2
$hadoop_conf_dir
define	1
$hadoop_conf_dir	7
personal	1
cluster's	1
located	3
arbitrary	2
choice	1
jobid	1
hadoop_conf_dir=$homemyclusterconf$pbs_jobid
to	1
ensure	3
multiple	1
same	7
supercomputer	1
doesn't	2
cause	1
all	9
step	1
others'	1
config	3
directories

step	1
3	4
myhadoopconfiguresh
the	1
myhadoopconfiguresh	9
(which	2
bin	1
install	1
directory)	1
extracts	1
information	1
(node	1
names	1
count	1
etc)	3
supercomputer's	2
manager	3
(torque	1
grid	2
engine	1
slurm	1
populates	1
($hadoop_conf_dir)	1
necessary	4
make	2
resources	1
assigned	1
by	8
manager

the	1
general	1
syntax	1
myhadoopconfiguresh

$	1
c	3
s	3
scratch$user$pbs_jobid
where

c	1
specifies	2
should	2
reside	2
provided	3
defined	2
environment	4
variable	2
recommended	2
above	2
always	1
specify	2
flag
s	1
scratch$user$pbs_jobid	1
location	5
compute	5
filesystem	9
cannot	1
filesystem
upon	1
created	2
populated	1
addition	2
hdfs	14
new	3
(in	1
specified	1
after	2
flag	2
node)	1
formatted

step	1
4	2
start	2
cluster
once	2
ready	1
go	1
once	2
again	1
startallsh	1
comes	2
(it's	1
$hadoop_homebin)

$	1
$hadoop_homebinstartallsh
warning	1
deprecated

starting	1
namenode	4
logging	2
scratchusername1234567masterlogshadoopusernamenamenodenode678sdsceduout
node678	1
warning	1
deprecated
node678
node678	1
starting	1
datanode	4
scratchusername1234567masterlogshadoopusernamedatanodenode678sdsceduout

the	1
warnings	1
about	1
being	1
deprecated	1
harmless	1
result	1
1x	1
020)	1
preferring	1
$hadoop_prefix	1
instead	1
find	1
extremely	1
annoying	1
add	2
hadoop_home_warn_suppress=true	1
~bashrc	1
suppress	1
it

to	1
verify	2
something	1
like

$	1
dfsadmin	1
report
configured	1
capacity	2
899767603200	1
(83797	1
gb)
present	1
899662729261	1
(83788	2
gb)
dfs	2
remaining	1
899662704640	1
24621	1
(2404	1
kb)
dfs	1
used%	1
0%
under	1
replicated	1
blocks	4
0
blocks	1
corrupt	1
replicas	1
0
missing	1
0


datanodes	1
available	1
(3	1
total	2
0	1
dead)

name	1
10510114650010

or	1
making	1
few	1
directories	2
loading	1
some	3
examples

$	1
dfs	4
data

$	2
wget	1
httpwwwgutenbergorgcacheepub2701pg2701txt

20140209	1
140112	1
(939	1
kbs)	1
	60
"pg2701txt"	1
saved	1
[12572741257274

$	1
put	2
pg2701txt	1
ls	1
data
found	1
items
rwrr	1
username	1
supergroup	1
1257274	1
20140209	1
1401	1
userusernamedatapg2701txt
and	1
wordcount	2
hadoop

$	2
jar	1
$hadoop_homehadoopexamples121jar	1
datapg2701txt	1
wordcountoutput
140209	1
140321	4
info	9
inputfileinputformat	1
input	1
paths	1
process	1
1
140209	1
utilnativecodeloader	1
loaded	1
nativehadoop	1
library
140209	1
warn	1
snappyloadsnappy	1
snappy	1
native	1
library	1
loaded
140209	1
mapredjobclient	7
job_201402091353_0001
140209	2
140322	1
map	4
0%	1
reduce	4
0%
140209	2
140327	1
100%	3
140334	1
33%
140209	1
140336	1
100%
140209	1
140337	2
complete	1
counters	1
29

and	1
output

$	1
cat	1
wordcountoutputpartr00000
"'a	1
3
"'also	1
1
"'are	1
1
"'aye	1
2

step	1
5	2
stop	3
done	1
stopallsh	2
stopallsh
stopping	1
jobtracker
gcn678	1
stopping	5
tasktracker
gcn785	1
tasktracker

stopping	1
namenode
gcn785	1
datanode
gcn678	1
datanode

gcn678	1
secondarynamenode
then	1
myhadoopcleanupsh	2
included	1
logfiles	1
off	1
jobtracker	1
(useful	1
debugging	1
failed	1
jobs)	1
delete	1
temporary	2
creates	4
over	7
node

$	1
myhadoopcleanupsh
copying	1
logs	1
back	1
homeusernamemyclusterconf1234567masterlogs

removed	1
`scratchusername1234567mastermapred_scratchtasktracker'
removed	1
`scratchusername1234567mastermapred_scratchttprivate'

strictly	1
speaking	1
neither	1
steps	2
(stopallsh	1
myhadoopcleanupsh)	1
since	1
clean	1
ends	2
hurt	1
two	1
explicitly	1
save	1
potential	2
headaches

advanced	1
features
persistent	1
mode
although	1
preferred	1
way	1
nodelocal	5
disk	1
drawback	1
state	3
persisting	1
(presumably)	1
purged	1
manager

to	1
address	1
limitation	1
provides	2
"persistent"	2
mode	3
whereby	1
datanodes	2
actually	2
stored	1
persistent	7
lustre)	2
linked	1
at	3
filesystem

to	1
configure	1
p	3
act	1
true	1
storage	8
backend	1
eg

myhadoopconfiguresh	1
pathtosharedfilesystem	5
\
	2
yournewconfigdir	2
pathtonodelocalstorage
in	1
case	1
accessible	1
(this	1
states)	1
pathtonodelocalstorage	2
remains	1
path	1
across	1
(eg	2
tmp)

persistent	1
under	2
stores	1
data	3
pathtosharedfilesystemnamenode_data	1
fsimage)	1
symlinks	1
pointing	3
point	1
filesystem

you	1
safely	1
mapreduce	2
shut	1
down	1
even	2
wipe	1
out	1
later	1
time	1
request	1
batch	2
scheduler	1
command	3
option	2
detect	1
existing	1
adjust	1
resulting	2
configurations	3
accordingly	1
mechanism	1
store	2
system

use	1
hadoop's	1
performance	2
resiliency	1
arises	1
fact	1
resides	1
physically	1
discrete	1
devices	1
datanodes'	1
device	2
(a	1
san	1
nfsmounted	1
lose	1
parallelism	2
perfomance	1
makes	1
useful	1
effect	1
shooting	1
yourself	1
foot	1
doing	1
this

the	1
only	1
exception	1
parallel	1
clustered	1
underneath	1
may	2
allow	1
recover	1
loss	1
object	1
targets	1
however	1
other	2
bottlenecks	1
limitations	1
enter	1
picture

ip	1
infiniband
myhadoop	1
simple	2
facility	1
traffic	1
ipoib	3
(ip	1
infiniband)	1
interfaces	1
specifying	1
followed	1
regular	1
expression	1
transformation	3
passed	1
sed)	1
list	1
hostnames	2
modified	2
hostnames

for	1
interface	1
"node01"	1
"node01ibnet"	1
be

i	1
's$ibnet'
environment	1
variables
the	1
commandline	2
options	1
via	3
variables	4
listed	1
below

switch	environment	1
variable	option
n	nodes	number	1
cluster
p	mh_persist_dir	location	1
data
c	hadoop_conf_dir	location	1
building	1
directory
s	mh_scratch_dir	location	1
data
h	hadoop_home	location	1
containing	1
$hadoop_homeconf
i	mh_ipoib_transform	regex	1
(passed	1
sed	1
e)	1
transform	1
ip	2
infiniband	2
hosts
the	1
order	1
precedence	1
is

etcmyhadoopconf	1
loaded
environment	1
loaded
command	1
line	1
switches	2
evaluated
etcmyhadoopconf
myhadoop	1
allows	1
systems	2
administators	2
sense	1
entire	2
myhadoopconf	3
"etcmyhadoopconf"	1
relative	1
myhhadoopconfiguresh	1
bash	1
lines	2
evaluated	2
before	1
builds	1
defaults	1
overridden	1
called

all	1
asis	1
significant	1
systemspecific	1
modification	1
made	1
wish	1
variables

mh_scratch_dir	1
sets	3
contain	1
eg	1
mh_scratch_dir=scratch$user
hadoop_home	1
propogate	1
through	1
process
mh_ipoib_transform	1
runs	1
interfaces
this	1
document	1
was	1
national	1
science	1
foundation	1
(nsf)	1
grant	1
0910812	1
indiana	1
university	1
"futuregrid	1
experimental	1
highperformance	1
testbed"	1
opinions	1
findings	1
conclusions	1
recommendations	1
expressed	1
material	1
author(s)	1
necessarily	1
reflect	1
views	1
nsf

basic	1
030b
patch	1
configuration
starting	1
cluster
step	2
$hadoop_home
step	1
$hadoop_conf_dir
step	1
myhadoopconfiguresh
step	1
cluster
advanced	1
features
last	1
october	1
20	1
2019
glenn@glennklockwoodcom	1
