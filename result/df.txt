pa	pitt
newy	ny
cali	sf
oregon	portland	1
author	1
abstract
patenting	1
in	18
software	7
cloud	14
computing	11
and	19
artificial	1
intelligence	3
has	12
grown	1
rapidly	2
recent	2
years	1
such	12
patents	1
are	17
acquired	1
primarily	2
by	14
large	6
us	2
technology	8
firms	1
as	17
ibm	4
microsoft	2
google	3
hp	1
well	5
japanese	1
multinationals	1
sony	1
canon	1
fujitsu	1
chinese	1
patenting	1
the	19
is	19
small	1
but	10
growing	5
worldleading	1
for	17
drone	1
machine	7
learning	4
seen	2
exponential	1
growth	2
since	6
2010	1
although	6
neural	1
networks	2
saw	1
a	19
strong	2
burst	1
of	19
activity	1
1990s	1
that	18
only	11
recently	1
been	6
surpassed	1
all	12
technological	2
fields	2
number	8
per	1
inventor	1
declined	1
nearmonotonically	1
except	1
increases	2
productivity	4
semiconductors	1
late	1
most	11
hightech	1
japan	1
country	1
outside	3
with	19
significant	5
however	13
whereas	2
played	1
an	17
important	7
role	3
network	6
it	18
not	16
involved	2
current	1
acceleration	1
comparing	2
periods	1
1970–89	1
2000–15	1
period	3
entrant	1
assignees	1
exception	2
conceptual	3
overview	2
mapreduce	5
hadoop
home
dataintensive	1
computing
hadoop
overview	1
hadoop
1	2
introduction
this	1
page	4
serves	2
30000foot	1
programming	2
paradigm	1
key	5
features	3
make	6
useful	4
solving	1
certain	1
types	3
workloads	1
simply	4
cannot	2
be	16
treated	1
using	13
traditional	5
parallel	3
methods	4
covers	1
broad	1
basics	3
intended	1
to	18
provide	7
you	10
information	9
need	12
follow	2
subsequent	1
pages	1
i've	4
written	3
on

writing	1
hadoop	5
applications	7
python	4
streaming
parsing	1
vcf	3
files	9
streaming
parallel	1
r	2
hadoop
2	1
parallelism
in	1
order	6
appreciate	1
what	11
brings	2
table	2
i	7
think	2
meaningful	1
contrast	1
call	3
problems	3
define	3
"traditional"	1
those	9
which	13
use	11
libraries	4
like	8
mpi	2
openmp	1
cuda	1
or	14
pthreads	1
produce	1
results	1
utilizing	2
multiple	7
cpus	2
perform	2
some	11
sort	3
numerical	1
calculation	1
concurrently	1
suited	1
being	9
solved	1
these	11
typically	2
share	3
two	8
common	1
features

they	1
cpubound	1
part	5
problem	3
takes	5
time	10
doing	4
calculations	1
involving	3
floating	1
point	5
integer	1
arithmetic
input	1
data	16
gigabytescale	1
necessary	4
describe	2
conditions	3
less	4
than	7
hundred	1
gigabytes	2
very	11
often	3
few	5
megabytes	1
at	8
most
item	1
#1	1
may	11
seem	1
trivial	1
after	6
computers	1
meant	3
compute	4
so	15
wouldn't	1
parallelized	1
fundamentally	1
limited	1
how	8
quickly	4
computer	4
can	15
do	9
calculations?

traditionally	1
answer	2
this	14
question	2
yes	1
landscape	1
changing	2
over	7
last	2
decade	1
sources	1
vast	1
unending	1
(eg	6
social	1
media	1
inexpensive	1
genenome	1
sequencing)	1
have	16
converged	1
highcapacity	1
hard	4
drives	1
advanced	2
filesystems	2
support	6
them	7
now	5
dataintensive	3
emerging	2
aforementioned	1
demonstrate	1
following	5
features

input	1
far	2
beyond	1
datasets	1
commonly	1
on	18
tens	1
hundreds	1
thousands	2
terabytes
they	1
iobound	1
longer	3
get	5
from	14
its	12
permanent	2
location	6
cpu	1
operate	1
data
21	1
applications
to	1
illustrate	2
differences	1
schematic	1
depicts	1
your	8
typical	1
traditionally	1
application	9
works

program	1
flow	1
problem
the	1
input	5
stored	7
remote	3
storage	6
device	7
(a	2
san	3
file	7
server	3
serving	1
nfs	1
lustre	1
gpfs	1
filesystem	3
etc	2
grey	1
cylinders)	1
resources	8
elements	1
(blue	1
boxes)	1
abstract	2
units	2
represent	2
ranks	1
nodes	3
threads	1
sharedmemory	1
system

upon	1
launching	1
application

a	1
master	2
worker	1
(mpi	1
rank	1
thread	1
etc)	3
reads	1
disk	4
(green	1
arrow)
the	1
then	7
divides	1
up	11
into	10
chunks	2
sends	2
parts	2
each	8
other	9
workers	2
(red	2
arrows)
all	1
their	13
chunk	1
data
all	1
communicate	2
continue	2
next	4
iteration	1
calculation
note	1
cases	1
io	1
api	3
mpiio	1
collectively	1
read	3
resides	3
must	5
highperformance	6
sustain	1
required	3
networkread	1
bandwidth

the	1
fundamental	3
limit	2
scalability	4
here	4
step	4
#1the	1
process	10
reading	3
arrow)	1
performed	1
serially	1
even	6
if	11
ingestion	1
separated	3
squares)	1
pipe	1
through	8
finite	1
rate	1
while	4
possible	3
increase	6
speed	3
connection	3
between	9
throwing	1
more	14
money	5
buying	2
fast	5
ssds	2
faster	2
networking	2
andor	1
servers)	1
cost	7
does	9
scale	5
linearly

22	1
applications
the	1
completely	1
different	13
way	10
subset	3
parallelizable	1
gets	3
around	2
bottleneck	1
ingesting	1
(that	2
pesky	1
green	1
parallelism	3
oppositeit	1
data

program	1
problem
in	1
separate	2
system	11
rather	4
exists	2
little	3
pieces	1
permanently	2
allows	5
our	5
procedure	1
steps

we	1
don't	3
move	3
any	10
predivided	1
already	6
capable	1
acting	1
elements
all	1
functions	1
sent	2
where	6
respective	1
exist	1
calculations
all	1
calculation
thus	1
needs	6
moved	2
when	11
communicating	1
#3	1
there	12
no	11
serial	1
loaded	4
before	8
distributed	4
because	13
resources

of	1
course	4
able	5
independent	1
principal	1
constraint	1
jobs	4
ideally	1
trivially	1
quantities	2
worker's	1
depend	1
will	14
begin	1
encounter	2
diminishing	1
returns

3	1
	16
implementation
now	1
we've	2
established	1
description	1
concept	1
bringing	1
we	8
equipped	1
look	3
actual	5
implementation	1
mapreduce

31	1
magic	1
hdfs
the	1
idea	4
underpinning	1
mapreducebringing	1
instead	8
oppositeshould	1
sound	1
simple	5
solution	4
inherent	1
devil	1
details	2
implementing	1
framework	2
single	5
transparently	1
diced	1
across	5
physical	2
(all	1
appearing	1
remain	1
user)	1
trivial

hadoop	1
perhaps	1
widely	2
used	11
accomplishes	1
feat	1
hdfs	5
provides	6
chunking	1
distribution	3
efficient	2
we're	4
talking	1
about	9
let's	1
refer	2
nodes

hdfs	1
copy	5
manner	1
unlike	2
many	8
commands	2
manipulating	1
(ls	1
mkdir	5
rm	1
mv	1
cp	2
cat	4
tail	1
chmod	1
name	4
few)	1
behave	1
might	3
expect	1
standard	3
linux's	1
ext4)

the	1
magical	1
going	6
just	6
underneath	2
surface	1
appears	1
contains	4
reality	2
nodes

schematic	1
depicting	1
hdfs
when	1
depicted	1
above	6
sliced	1
64	2
mb	2
"chunks"	1
replicated	2
three	4
times	2
reliability	1
various	2
cluster	4
given	2
physically	2
chunked	1
triplicate	1
interactions	1
still	6
appear	2
same	6
copied	1
initially	1
thus	4
handles	1
burden	1
slicing	1
distributing	1
recombining	1
you

hdfs's	1
size	4
replication
the	1
(block)	1
choice	2
replicate	1
hdfs's	1
default	4
values	3
decisions	1
changed

the	1
block	1
modified	5
dfsblocksize	1
property	1
hdfssitexml	1
128	1
production	2
environments
the	1
replication	1
factor	2
dfsreplication	1
also	14
changed	1
perfile	1
basis	2
specifying	2
d	3
dfsreplication=1	1
put	3
command	6
line	7
dfs	5
setrep	1
w	1
1	7
command
32	1
jobs
hdfs	1
interesting	1
automatic	3
recovery	1
userspace	1
relatively	1
easy	6
configure	5
conceptually	4
understand	4
true	2
utility	1
comes	7
light	1
executed	2
hdfs

as	1
implies	1
principally	1
comprised	1
steps	5
map	5
reduce	9
overall	2
workflow	2
generally	2
looks	2
something	5
this

program	1
application
the	1
left	1
half	1
diagram	1
described	2
previous	4
section	2
copyfromlocal	4
automatically	3
moving	2
strictly	2
job	5
(ie	1
dataset	1
home	3
would	8
filesystem)	1
job's	1
started

321	1
step
once	1
initiated	1
step

launches	1
mappers	4
contain	3
data
for	1
mapper	3
"splits"	1
individual	2
lines	5
text	3
newline	1
characters	1
(\n)
each	1
split	2
(line	1
was	9
terminated	2
\n)	1
function
your	1
function	3
expected	1
turn	1
zero	3
keyvalue	3
pairs	3
"emit"	1
step
that	1
step's	1
transform	3
raw	2
series	2
expectation	1
parsed	1
analyzed	1
meaningfully	1
it's	4
perfectly	1
fine	1
duplicate	2
keys	5
emitted	1
mappers

the	1
decision	1
along	2
behavior	1
assumes	3
ascii	1
change	7
passed	3
alternate	1
inputformats

322	1
finished	2
digesting	1
sorted	2
according	2
reducers	3
sharing	4
always	7
go	7
reducer	3
corollary	1
one	12
particular	4
specific	4
guaranteed	2
continuous	1
strip	1
received

your	1
based	4
example	10
calculate	1
sum	1
word	2
count	3
example)	1
emit	1
back	6
unique	3
keys'	1
result	3
function's	1
calculation

the	1
shuffle
the	1
sorting	1
mapper's	1
output	4
called	3
"shuffle"	1
really	4
happens	2
partitioner	1
determine	3
they	10
to

the	1
hashes	1
modulus	1
hash	1
pair	2
value	4
therefore	1
wind	1
reducer

once	1
assigned	3
loop	2
reducer's	1
examine	1
see	7
tutorial	3
writing	4
follows	1
essential	3
streaming	3
interface

this	1
complicated	2
without	7
sample	2
code	6
easier	3
working	7
example

4	1
summary
this	1
admittedly	1
dry	1
accompany	1
points	3
should	7
take	7
away

mapreduce	1
resources
hadoop	1
storing	5
fashion	1
hdfs
hdfs	1
stores	2
nodes
hdfs	1
presents	1
users	10
despite	2
fact
mapreduce	1
ideal	1
operating	5
flat	1
(unstructured)	1
operations	2
them
hadoop	1
stage	1
where
the	1
transforms	2
occur
the	1
value
if	1
interest	2
remaining	2
having	8
strongly	1
recommend	2
looking	4
my	4
much	10
material	4
context	3
(counting	1
text)	1
python

1	1
introduction
2	3
parallelism
21	1
applications
22	1
applications
3	1
implementation
31	1
hdfs
32	1
jobs
4	1
summary
last	1
october	4
20	4
2019
glenn@glennklockwoodcom	4
streaming
home
dataintensive	2
computing
hadoop
hadoop	2
python
1	1
introduction
one	1
unappetizing	1
aspects	2
hpc	3
java	2
designed	4
language	3
definitively	1
speak	1
myself	1
suspect	1
high	4
priority	2
domain	1
scientists	1
turns	1
out	10
though	3
write	4
want	6
interface	4
feature	1
making	4
palatable	1
scientific	1
community	2
means	3
turning	1
existing	2
perl	1
script	4
require	2
derived	1
hadoopcentric	1
languages	2
pig	1
guide	4
illustrates	2
run	8
entirely	3
ever	1
mess	2
xsede's	2
gordon	3
resource	10
sdsc

once	1
running	6
pythonbased	1
covered	1
practical	1
parse	2
variant	1
format	4
(vcf)	2
parsing	2
library	3
install	4
root	2
privileges	2
supercomputing	1
account

the	1
wordcount	4
github	2
account	1
got	2
implementations	1
posted	1
there

2	1
review	1
hpc
this	2
familiar	1
level	2
good	4
shape	1
have

this	1
(supercomputer)	1
mean

using	1
noninteractive	2
allencapsulated	1
setup	1
teardown	1
user	6
guide's	1
hadoop
creating	1
semipersistent	3
submitting	1
hand	2
clusters	3
supercomputers
for	1
purposes	5
keeping	1
specifically	2
clear	1
assume	2
spun	2
relevant	1
route	2
submit	2
wraps	1
job

3	1
canonical	1
example
counting	1
words	1
document	3
"hello	1
world"	1
among	3
simplest	1
full	2
map+reduce	1
recalling	1
desired	1
act	3
counting	1
as

the	1
(i	1
herman	1
melville's	1
classic	1
moby	2
dick)	1
convert	1
(words)	1
1
the	1
combine	2
adding	2
every	3
(word)	1
list	4
corresponding	1
key's	1
(word's)	1
count
schematic	1
mapreduce
with	1
program	3
acts	1
inputoutput	1
streams	1
equivalent	1
pipes

$	1
inputtxt	1
|	1
mapperpy	1
reducerpy	1
>	4
outputtxt
that	1
via	5
stdin	1
mapped	1
stdin

31	1
mapper
the	1
quite	3
implement	1
this

#usrbinenv	1
python

import	1
sys

for	1
sysstdin
	1
=	1
linestrip()
	1
linesplit()
	1
keys
	1
1
	1
print(	1
"%s\t%d"	1
%	1
(key	1
value)	1
)
where

hadoop	1
("line"	1
defined	4
string	2
linefeed	1
character	1
\n)
python	1
strips	1
leadingtrailing	1
whitespace	1
(linestrip()
python	1
splits	2
(linesplit())
for	1
(which	3
become	4
key)	1
assign	1
print	1
tab	1
(\t)
a	1
detailed	1
explanation	1
found	3
yahoo's	1
excellent	2
tutorial

32	1
shuffle
a	1
lot	7
largely	2
transparent	1
developer	1
brief	2
transformed	1
(termed	1
shuffle	2
step)	1
that

all	1
presented	1
function
all	1
reducer
these	1
because

as	1
processed	1
know	4
never	1
again
if	1
gain	3
parallelization	1
come	5
happens
33	1
reducer
the	1
keypairs	1
apply	2
logic

if	1
key
	1
add	4
total
otherwise
	1
total
	1
reset	1
total	3
0
	1
and
	1
"this	1
key"	1
considered	4
"previous	1
key"
translating	1
extra	4
tighten	1
logic	1
get

#usrbinenv	1
sys

last_key	1
none
running_total	1
0

for	1
input_line	1
input_linestrip()
	1
this_key	1
input_linesplit("\t"	1
1)
	1
int(value)

	1
last_key	1
==	1
this_key
	1
running_total	1
+=	1
value
	1
else
	1
last_key
	1
(last_key	1
running_total)	1
)
	1
this_key

if	1
)
34	1
job
if	1
reducing	1
first	3
download	5
(moby	1
load	3
purposely	1
am	4
renaming	1
mobydicktxt	1
original	3
pg2701txt	3
highlight	1
file

$	1
wget	4
httpwwwgutenbergorgcacheepub2701pg2701txt
$	1
wordcount
$	1
wordcountmobydicktxt
you	1
verify	2
properly

$	1
ls	3
wordcountmobydicktxt
found	1
items
rwrr	3
2	7
glock	2
supergroup	3
1257260	1
20130717	1
1324	1
userglockwordcountmobydicktxt
before	1
sure	2
scripts	3
actually	4
work	11
matter	3
pipes	1
bit	3
1000	1
dick)

$	1
head	2
n1000	1
reducerpy

young	1
4
your	1
16
yourself	1
3
zephyr	1
1
once	1
mapperreducer	2
errors	1
plug	2
accomplish	1
jar	4
hadoopstreamingxyzjar	1
apache	2
$hadoop_homecontribstreaming	1
$hadoop_home	2
base	1
directory	3
installation	3
xyz	1
version	4
opthadoopcontribstreaminghadoopstreaming103jar	2
launch	1
like

$	2
\
	3
"python	1
$pwdmapperpy"	1
$pwdreducerpy"	1
"wordcountmobydicktxt"	1
"wordcountoutput"

packagejobjar	1
[scratchglock819550gordonfe2localhadoopglockdatahadoopunjar4721749961014550860	1
[	1
tmpstreamjob7385577774459124859jar	1
tmpdir=null
130717	1
192616	1
info	3
utilnativecodeloader	2
nativehadoop	2
library
130717	1
warn	2
snappyloadsnappy	2
snappy	2
native	2
loaded
130717	1
mapredfileinputformat	1
paths	3
1
130717	1
streamingstreamjob	1
getlocaldirs()	1
[scratchglock819550gordonfe2localhadoopglockdatamapredlocal
130717	1
job_201307171926_0001
130717	1
kill	1
run
130717	1
opthadooplibexecbinhadoop	1
dmapredjobtracker=gcn1334ibnet054311	1
tracking	1
url	1
httpgcn1334ibnet050030jobdetailsjsp?jobid=job_201307171926_0001
and	1
"tracking	1
url"	1
deceptive	1
probably	2
won't	2
access	6
fortunately	2
commandline	2
monitoring	1
somewhat	1
similar	1
qstat	2
noting	1
jobid	3
(highlighted	1
above)	1
do

$	1
status	1
job_201307171926_0001
job	1
job_201307171926_0001
file	1
hdfsgcn1334ibnet054310scratchglock819550gordonfe2localhadoopglockdatamapredstagingglockstagingjob_201307171926_0001jobxml
tracking	1
httpgcn1334ibnet050030jobdetailsjsp?jobid=job_201307171926_0001
map()	1
completion	1
10	3

reduce()	1
10

counters	1
30
	1
counters
	1
launched	2
tasks=1
	1
slots_millis_maps=16037
	1
spent	1
reduces	2
waiting	3
reserving	1
slots	1
(ms)=0
	1
maps	1
tasks=2
	1
datalocal	1
tasks=2

since	1
runs	2
foreground	1
another	3
terminal	1
(with	1
hadoop_conf_dir	2
properly	2
exported)	1
check	3
metrics	1
highlighted	3
task	2
tasks	7
nodes

35	1
adjusting	1
parallelism
unlike	1
necessarily	4
ultimately	1
determined	1
nature	1
due	4
distributes	1
"suggest"	1
applying	2
highlighted

$	1
mapredmaptasks=4	2
wordcountmobydicktxt	1
wordcountoutput

$	1
job_201307172000_0001

	1
slots_millis_maps=24049	1

	9
racklocal	1
tasks=4
	1
tasks=3
similarly	1
mapredreducetasks=4	1
suggest	1
flexible	4
set	5
file

with	1
said	5
fact	2
defaults	2
says	1
trying	4
solvethat	1
entire	4
stupid	1
concepts	1
neatly	1
12	1
waste	1
done	6
assigns	1
increments	1
handle	2
multigigabyte	1
getting	4
research	2
requires	4
knowledge	2
above

to	1
fill	1
gaps	1
shows	2
applied	1
solve	1
realworld	1
exotic	1
notcompletelyuniform	1
files

this	1
developed	4
national	4
science	4
foundation	4
(nsf)	4
under	5
grant	4
0910812	3
indiana	4
university	5
"futuregrid	3
experimental	4
grid	4
testbed"	3
opinions	3
findings	3
conclusions	3
recommendations	3
expressed	3
author(s)	3
reflect	3
views	3
nsf

1	2
hpc
3	1
example
31	1
mapper
32	1
shuffle
33	1
reducer
34	1
job
35	1
parallelism
last	1
computing
hadoop
parsing	1
vcfs	1
introduction
variant	1
type	5
variations	1
within	5
genome	1
anything	3
genomic	1
dozens	1
compressed)	1
converting	1
yet	2
dataintensive

this	1
were	2
missing	1
example

it	1
uses	2
installed
this	1
installed	2
nonroot	1
nondefault	1
python
this	1
way
this	1
builds	2
upon	2
against	2
guides	1
supporting	3
sdsc	2
futuregrid	2
testing	2
development	6
wide	1
range	2
minimal	1
modification

2	1
overview
files	1
perfect	1
analysis	1
doesn't	3
form	1
propose	1
pipeline	1
red

proprocessing	1
data
mapping
shuffling
reducing
postprocessing	1
data
within	1
preprocessing	1
header	1
describes	2
rest	2
(or	2
reducers)	1
reader	1
sense	2
fed	1
involves	2
top	3
printing	1
stdout	1
until	2
nonheader	1
reached

schematic	1
preprocessing
we	1
printed	1
bother	1
messing	1
file's	1
contents	2
obviates	1
writing)	2
too	2
big	4
proprocessing	1
leaves	2
(headertxt)	1
untouched	1
file

the	1
postprocessing	1
dependent	1
area	2
structured	2
could	4
fromed	1
postgresql	1
database	6
taking	1
connecting	3
postgres	1
froming	1
handled	3
step

3	1
nonroot
python	1
makes	6
virtualenv	1
end	3
essentially	2
gives	5
behaves	1
administrative	2
sets	3
oneline	1
enable	3
disable)	1
use

the	1
setting	1
unpack	2
it

$	1
httppypipythonorgpackagessourcevvirtualenvvirtualenv1712targz
$	1
tar	2
xzf	1
virtualenv1712targz
you	1
decide	2
prefix	1
personal	4
pick	1
~mypython	1
arbitrary	2
project	5
either	5
execute	1
virtualenvpy	1
unpacked	1
tell	2
camp	1
forget	2
python273	1
module	2
python27	1
systemdefault	1
python24

$	1
python273
$	1
virtualenv1712virtualenvpy	1
~mypython
this	1
create	6
bunch	2
including

$homemypythonbin
$homemypythonbinpython	1
executable	1
knows	1
find	5
installing
$homemypythonbinpip	1
libraries
$homemypythonbinactivate	1
you'll	1
source	1
activate	1
virtual	7
environment
$homemypythonlib	1
installed
then	1
"enable"	1
virutal	1
environment	7
sourcing	2
log	2
remember	1
first

$	1
~mypythonbinactivate
your	1
prompt	1
indicate	1
you're	2
active	1
pip	1
pyvcf	1
library

(mypython)$	1
pyvcf
downloadingunpacking	1
pyvcf
	1
downloading	2
pyvcf064targz
	1
setuppy	1
egg_info	1
package	2
pyvcf

successfully	1
pyvcf
cleaning	1
up
now	1
confirm	1
accessible	3
virtualenv'ed	1
python

(mypython)$	1
python
python	1
273	1
(default	1
feb	1
7	1
2013	1
211153)	1

[gcc	1
412	1
20080704	1
hat	2
41250)	1
linux2
type	1
"help"	1
"copyright"	1
"credits"	1
"license"	1
information
>>>import	1
vcf
>>>
4	1
mapper
this	1
mapper+reducer	1
github

5	1
reducer
this	1
github

6	1
launch
there	1
additions	1
made	4
changes	1
red	2
below

$	1
$homemypythonbinpython	1
$pwdparsevcfpy	1
b	1
patientdatavcf	1
headertxt
$	1
data
$	1
mapredreducetasks=0	1
"$homemypythonbinpython	1
m	1
headertxt030"	1
">python	1
r"	1
"datapatientdatavcf"	1
"dataoutput"	1
cmdenv	1
ld_library_path=$ld_library_path
the	1
following

module	1
provided	3
(27x)	1
(24x)
python	1
added	1
extract	1
file
d	1
optional	1
tells	1
(but	1
get)	1
four	2
mappers
d	1
disables	1
anything
we	1
explicitly	2
specify	3
path	3
custom	1
($homemypythonbinpython	1
)	1
calling	1
ensure	7
headertxt030	1
expects	1
commaseparated	1
containing
the	1
containing	2
vcf's	1
(headertxt)
the	1
allele	1
frequency	1
record	1
(030)
we	1
mapredreducetasks=0
cmdenv	1
ld_library_path=$ld_library_path	1
critical	6
passes	1
environment's	1
$ld_library_path	1
variable	3
execution	2
issued	2
option	5
ensures	2
propagated	1
options	5
propagate	1
variables	4
mappersreducers	1
need
7	1
scaling	1
behavior
this	1
yet

this	1
overview
3	1
nonroot
4	1
mapper
5	1
reducer
6	1
launch
7	1
behavior
last	1
clusters
home
dataintensive	1
instructions	1
myhadoop	2
01	1
vastly	1
simplfied	1
release	1
02	1
placeholder	1
outofdate	1
supports	1
spark	2
2x	1
mentioned	6
interested	3
uptodate	2
please	2
contact	2
me	2
prioritize	1
updating	1
it

this	1
following

users	1
allowed	1
ssh	2
allocated	2
password
users	1
open	1
ports	2
1024	1
jobs'	1
nodes
each	1
node	3
local	4
scratch	2
space	3
shared	2
nodes†	1
it
for	1
sake	1
simplicity	1
stresses	1
(like	1
io)	1
allocation	1
nonhadoop	1
introduces	1
unnecessary	1
complexity
we	1
installing	4
opposed	1
distinction	1
architecturally	1
(and	1
configurationally)	1
widespread	1
today	1
ecosystem	3
future	4
incorporating	1
hadoop2yarn	1
yet

the	1
exact	1
latest	2
(as	1
121	1
reason	1
earlier	2
versions	2
tested	1
104	1
111	1
ancient	1
020	1
releases

†	1
own	3
userwriteable	1
requirement	2
deploying	2
supercomputers	2
try	1
cover	2
topic	1
sharedparallel	1
guide

basic	1
installation
install	1
030b
you	1
tarballs	1
locations

hadoop121bintargz
myhadoop
neither	1
proper	3
"installation"	1
wherever	1
you'd	1
eg

$	1
~hadoopstack
$	1
cd	1
zxvf	1
hadoop121bintargz
hadoop121
hadoop121eclipsetemplates
hadoop121eclipsetemplatesexternaltoolbuilders
hadoop121eclipsetemplateslaunches
hadoop121bin
hadoop121c++


$	1
myhadoop030btargz
myhadoop030b
myhadoop030bchangelog
myhadoop030blicense
myhadoop030breadmemd
myhadoop030bbin
myhadoop030bbinmyhadoopbootstrapsh

patch	1
configuration
myhadoop	1
ships	1
patch	1
myhadoop121patch	1
converts	1
configuration	2
ship	1
templates	1
modify	1
wants	1
patch

$	1
hadoop121conf
$	1
<	1
myhadoop030bmyhadoop121patch
patching	1
coresitexml
patching	1
hdfssitexml
patching	1
mapredsitexml
and	1
that's	1
cluster

starting	1
cluster
when	1
spin	2
environmenteither	1
interactive	1
job

step	1
$hadoop_home
define	1
section's	1
$homehadoopstackhadoop121

$	1
export	2
hadoop_home=$homehadoopstackhadoop121
myhadoop	1
$hadoop_homeconf	1
patched	1
build	1
configurations

adding	1
$hadoop_homebin	1
$path	1
additional	1
convenience

$	1
path=$hadoop_homebin$path
$	1
path=$homehadoopstackmyhadoop030bbin$path
in	1
event	2
$java_home	1
that

$	1
java_home=usrjavalatest
step	1
choose	2
$hadoop_conf_dir
define	1
$hadoop_conf_dir	2
cluster's	2
located	2
hadoop_conf_dir=$homemyclusterconf$pbs_jobid
to	1
supercomputer	2
cause	1
others'	1
config	3
directories

step	1
3	4
myhadoopconfiguresh
the	1
myhadoopconfiguresh	1
bin	1
directory)	1
extracts	1
(node	1
names	2
supercomputer's	2
manager	3
(torque	2
engine	2
slurm	1
populates	2
($hadoop_conf_dir)	1
manager

the	1
general	3
syntax	1
myhadoopconfiguresh

$	1
c	2
s	1
scratch$user$pbs_jobid
where

c	1
specifies	1
reside	1
recommended	1
flag
s	1
scratch$user$pbs_jobid	1
filesystem
upon	1
created	4
populated	1
addition	6
new	7
(in	2
specified	2
flag	1
node)	1
formatted

step	1
4	3
start	2
cluster
once	2
ready	2
once	2
again	3
startallsh	1
(it's	1
$hadoop_homebin)

$	1
$hadoop_homebinstartallsh
warning	1
deprecated

starting	1
namenode	2
logging	1
scratchusername1234567masterlogshadoopusernamenamenodenode678sdsceduout
node678	1
warning	1
deprecated
node678
node678	1
starting	1
datanode	1
scratchusername1234567masterlogshadoopusernamedatanodenode678sdsceduout

the	1
warnings	1
deprecated	1
harmless	1
1x	2
020)	1
preferring	1
$hadoop_prefix	1
extremely	2
annoying	2
hadoop_home_warn_suppress=true	1
~bashrc	1
suppress	1
it

to	1
dfsadmin	1
report
configured	1
capacity	3
899767603200	1
(83797	1
gb)
present	1
899662729261	1
(83788	1
gb)
dfs	1
899662704640	1
24621	1
(2404	1
kb)
dfs	1
used%	1
0%
under	1
blocks	1
0
blocks	1
corrupt	1
replicas	1
0
missing	1
0


datanodes	1
available	3
(3	1
0	2
dead)

name	1
10510114650010

or	1
directories	2
loading	1
examples

$	1
data

$	1
httpwwwgutenbergorgcacheepub2701pg2701txt

20140209	1
140112	1
(939	1
kbs)	1
"pg2701txt"	1
saved	2
[12572741257274

$	1
data
found	1
username	1
1257274	1
20140209	1
1401	1
userusernamedatapg2701txt
and	1
hadoop

$	1
$hadoop_homehadoopexamples121jar	1
datapg2701txt	2
wordcountoutput
140209	1
140321	1
inputfileinputformat	2
1
140209	1
library
140209	1
loaded
140209	1
mapredjobclient	2
job_201402091353_0001
140209	1
140322	1
0%	2
0%
140209	1
140327	1
100%	3
140334	1
33%
140209	1
140336	1
100%
140209	1
140337	1
complete	3
counters	1
29

and	1
output

$	1
wordcountoutputpartr00000
"'a	2
3
"'also	2
1
"'are	2
1
"'aye	2
2

step	1
5	2
stop	1
stopallsh	1
stopallsh
stopping	1
jobtracker
gcn678	1
stopping	1
tasktracker
gcn785	1
tasktracker

stopping	1
namenode
gcn785	1
datanode
gcn678	1
datanode

gcn678	1
secondarynamenode
then	1
myhadoopcleanupsh	1
included	2
logfiles	2
off	2
jobtracker	2
(useful	1
debugging	2
failed	1
jobs)	1
delete	1
temporary	2
creates	1
node

$	1
myhadoopcleanupsh
copying	1
logs	2
homeusernamemyclusterconf1234567masterlogs

removed	1
`scratchusername1234567mastermapred_scratchtasktracker'
removed	1
`scratchusername1234567mastermapred_scratchttprivate'

strictly	1
speaking	2
neither	1
(stopallsh	1
myhadoopcleanupsh)	1
clean	2
ends	1
hurt	1
save	3
potential	1
headaches

advanced	1
features
persistent	1
mode
although	1
preferred	1
nodelocal	1
drawback	1
state	1
persisting	1
(presumably)	1
purged	1
manager

to	1
address	2
limitation	1
"persistent"	1
mode	1
whereby	1
datanodes	1
persistent	1
lustre)	1
linked	1
filesystem

to	1
p	1
backend	1
eg

myhadoopconfiguresh	1
pathtosharedfilesystem	1
yournewconfigdir	1
pathtonodelocalstorage
in	1
case	3
(this	1
states)	1
pathtonodelocalstorage	1
remains	1
tmp)

persistent	1
pathtosharedfilesystemnamenode_data	1
fsimage)	1
symlinks	1
pointing	1
filesystem

you	1
safely	1
shut	2
down	7
wipe	1
later	1
request	3
batch	2
scheduler	1
detect	2
adjust	1
resulting	1
configurations	1
accordingly	2
mechanism	1
store	3
system

use	1
hadoop's	2
performance	6
resiliency	1
arises	1
discrete	1
devices	5
datanodes'	1
nfsmounted	1
lose	1
perfomance	1
effect	1
shooting	1
yourself	2
foot	1
this

the	1
clustered	2
allow	2
recover	1
loss	3
object	1
targets	1
bottlenecks	1
limitations	1
enter	1
picture

ip	1
infiniband
myhadoop	1
facility	2
traffic	4
ipoib	1
(ip	1
infiniband)	1
interfaces	2
followed	1
regular	2
expression	1
transformation	2
sed)	1
hostnames	2
hostnames

for	1
"node01"	1
"node01ibnet"	1
be

i	1
's$ibnet'
environment	1
variables
the	1
listed	1
below

switch	environment	1
variable	option
n	nodes	number	1
cluster
p	mh_persist_dir	location	1
data
c	hadoop_conf_dir	location	1
building	1
directory
s	mh_scratch_dir	location	1
data
h	hadoop_home	location	1
$hadoop_homeconf
i	mh_ipoib_transform	regex	1
(passed	1
sed	2
e)	1
ip	1
infiniband	2
hosts
the	1
precedence	1
is

etcmyhadoopconf	1
loaded
environment	1
loaded
command	1
switches	2
evaluated
etcmyhadoopconf
myhadoop	1
systems	5
administators	1
myhadoopconf	1
"etcmyhadoopconf"	1
relative	1
myhhadoopconfiguresh	1
bash	2
evaluated	1
overridden	1
called

all	1
asis	1
systemspecific	1
modification	1
wish	1
variables

mh_scratch_dir	1
eg	1
mh_scratch_dir=scratch$user
hadoop_home	1
propogate	1
process
mh_ipoib_transform	1
interfaces
this	1
nsf

basic	1
030b
patch	1
configuration
starting	1
cluster
step	1
$hadoop_home
step	1
$hadoop_conf_dir
step	1
myhadoopconfiguresh
step	1
cluster
advanced	1
features
last	1
gordon
home	1
gordon

as	1
july	1
11	2
2014	1
diego	1
center	1
site	1
updated	2
maintaining	3
here

table	1
contents
introduction
the	1
script
set	1
variables
set	1
&	1
configs
format	1
hdfs
start	1
hadoop
further	1
niceties
final	1
script
using	1
cluster
shutting	1
cluster
introduction
on	1
dynamically	1
provision	1
quick	3
explains	1
endtoend	1
transient	1
terminate	1
flexible

for	1
smaller	2
analyses	1
issues	2
establish	1
right	3
feasibly	1
amazon	4
ec2	1
costs	3
(unlike	1
xsede	1
free)	1
wanted	1
prototype	1
learn	1
sdsc's	1
mean	2
long	2
lifetime	1
remotely	3
login	1
jobs

i've	1
both	4
xsedesdsc	1
several	1
machines	4
preconfigured	1
beautifully	1
portable	2
wrappers	1
utilize	1
portability	1
released	1
newer	1
accompanying	1
simplify	1
task

in	1
remainder	1
term	4
collection	2
jobs

the	1
script
this	1
spinning	1
things

set	1
needed	3
(path	1
java_home)	1
provided
set	1
(my_hadoop_home)
set	1
(handled	1
myhadoop)
set	1
structure	2
populate	1
userland	1
(also	1
myhadoop)
interface	1
figure	1
(masters	1
slaves	1
files)
establish	1
(hadooptmpdir	1
(hadoop_log_dir)
make	1
files
format	1
filesystem
spin	1
slave	1
service	11
service
designing	1
managers	1
follows

step	1
#1#3
export	1
my_hadoop_home="opthadoopcontribmyhadoop"
source	1
$my_hadoop_homebinsetenvsh
you	1
environment

the	1
my_hadoop_home	1
silly	1
(sourcing	1
setenvsh	1
things	2
variables

my_hadoop_home	1
template	1
launch
hadoop_home	1
installations	1
executables	1
are
hadoop_data_dir	1
determines	1
sit	1
sitting	1
node
on	1
(scratch$user$pbs_jobid)	1
finditself
on	1
hotel	1
tb	1
(scratchlocal$userdata)
on	1
sierra	1
currently	2
tmp	1
disk
hadoop_log_dir	1
(including	1
errors)	1
failures	1
impossible	1
kept	1
here
you	1
overwrite	1
systemwide	1
hadoop_data_dir	1
hadoop_log_dir	1
skip	1
directly	1
script

step	1
#4
export	1
hadoop_conf_dir=$pbs_o_workdir$pbs_jobid
export	1
pbs_nodefilez=$(mktemp)
sed	1
e	1
's$ibnet0g'	1
$pbs_nodefile	1
$pbs_nodefilez
$my_hadoop_homebinconfiguresh	1
n	1
||	1
exit	1
1
at	1
core	1
configuresh	1
#1#3	1
merges	1
together	2
customized	1
defines	1
literally	1
everything	5
supercomputer

hadoop_conf_dir	1
variable's	1
controlling	2
whenever	1
usually	5
give	3
static	1
home$userconfig	1
gordon)	1
id	1
($pbs_jobid	1
permutation	1
thereof)	1
spawn	1
simultaneously	1
stomping	1
other

pbs_nodefilez	1
gordonspecific	1
lists	1
tcp	1
associated	1
pbs	1
sge	1
"ibnet0	1
gcn1421ibnet0	1
gcn1421)	1
pbs_nodefilez	1
sidebar	1
below	1
generic	1
way

once	1
pbsconfiguresh	1
sgeconfiguresh	1
else)	1
following

creates	1
$hadoop_conf_dir
copies	1
$hadoop_conf_dir
defines	1
'master'	1
$hadoop_conf_dirmasters	1
$hadoop_conf_dirslaves	1
(torquepbsslurmsgeetc)
defines	1
correct	1
mapredsitexml	1
coresitexml
defines	1
$hadoop_data_dir	1
$hadoop_log_dir	1
hadoopenvsh
ssh's	1
nukes	1
nothing
a	1
safeguard	1
(perhaps	1
unnecessary)	1
built	3
nodes)	1
i'm	1
why	2
parameter	1
cluster

tcp	1
infiniband
if	1
configured	1
normal	2
020a	1
$pbs_nodefilez	1
sierra)	1
this

old_pbs_nodefile=$pbs_nodefile	1

export	1
pbs_nodefile=$(mktemp)	1

sed	1
$old_pbs_nodefile	1

$my_hadoop_homebinpbsconfiguresh	1
pbs_nodefile=$old_pbs_nodefile
that	1
is

make	1
backup	3
torque
append	1
ib	1
suffix	1
contained	1
referred	1
ibnet0	1
ib
run	1
sed
after	1
backedup	1
place	2
torque	1
severely	1
completes	1
this
step	1
#5
after	1
formatted	1
accept	1
operate

$hadoop_homebinhadoop	1
format
step	1
#6
finally	1
itself	1
cluster

$hadoop_homebinstartallsh
	1

sleep	1
$((12*3600180))
if	1
replacing	1
sleep	1
(sleep	1
$((12*3600180))	1
causes	2
hang	1
twelve	1
hours	2
minutes	1
teardown)	1
stays	1
further	1
instruction

further	1
niceties
at	1
bits	1
fully	1
functioning	1
script

#pbs	1
l	1
nodes=4ppn=1native
#pbs	1
walltime=120000
#pbs	1
q	1
normal
finally	1
gave	1
ourselves	1
180	1
seconds	1
forcibly	1
kills	1
like

$hadoop_homebinstopallsh
cp	1
lr	1
$pbs_o_workdirhadooplogs$pbs_jobid
$my_hadoop_homebincleanupsh	1
$hadoop_conf_dir
which	1
do

$hadoop_homebinstopallsh	1
nodes
the	1
submitted	1
($pbs_o_workdir)
the	1
cleanupsh	1
possibly	1
pbscleanupsh)	1
destroys	1
nodes
these	1
final	1
aren't	1
destroy	1
happen	1
practice	1
measure	1
courtesy	1
itself

the	1
script
when	1
gordon

#binbash
#pbs	1
hadoopcluster
#pbs	1
normal
#pbs	1
j	1
oe
#pbs	1
o	1
hadoopclusterlog
#pbs	1
v
	1
my_hadoop_home="opthadoopcontribmyhadoop"
export	1
hadoop_home="opthadoop"
export	1
hadoop_conf_dir=$pbs_o_workdir$pbs_jobid
	1
's$ibnet0'	1
$pbs_o_workdirhadoophoststxt
export	1
pbs_nodefilez=$pbs_o_workdirhadoophoststxt
	1

$my_hadoop_homebinconfiguresh	1
$hadoop_conf_dir
	1
's^export	1
hadoop_pid_dir=*export	1
hadoop_pid_dir=scratch'$user''$pbs_jobid''	1
$hadoop_conf_dirhadoopenvsh
sed	1
tmpdir=*export	1
tmpdir=scratch'$user''$pbs_jobid''	1
$hadoop_conf_dirhadoopenvsh
	1

$hadoop_homebinhadoop	1
format
	1

$hadoop_homebinstartallsh
	1
$((12*3600180))
	1

$hadoop_homebinstopallsh
cp	1
$hadoop_conf_dir
the	1
gnarly	1
(mouseover	1
explanation)

if	1
readytouse	1
unmodified	1
for

futuregrid	1
infiniband
futuregrid	1
infiniband
xsedesdsc	1
above
using	1
(determine	1
u	1
$user)	1
hadoop_conf_dir=$homehadoop123456gordonfe2local	1
(where	1
123456gordonfe2local	1
submitted)	1
command

$	1
hadoop_conf_dir=homeglockhadoop123456gordonfe2local
$	1
data
	1
#	1
dick
$	1
httpwwwgutenbergorgcacheepub2701pg2701txt
	1
hdfs
$	1
it
$	1
opthadoophadoopexamples103jar	1
wordcountoutput
130620	1
181736	1
1
130620	1
job_201306201808_0001
130620	1
181737	1
0%
130620	1
181751	1
181803	1
100%
130620	1
181808	1
job_201306201808_0001

$	1
wordcountoutput
found	1
20130620	1
1818	1
userglockwordcountoutput_success
drwxrxrx	1
1817	1
userglockwordcountoutput_logs
rwrr	1
366674	1
userglockwordcountoutputpartr00000
$	1
2
"'aye?	1
1

	1
real	1
filesystem
$	1
copytolocal	1
wordcountoutputpartr00000	1
mobydickout
if	1
path=opthadoopbin$path)

shutting	1
cluster
as	1
qdel	1
bottom	2
using

$	1
opthadoopbinstopallsh
$	1
opthadoopcontribmyhadoopbinpbscleanupsh	1
$hadoop_conf_dir
$	1
123456
acknowledgments	1
outlook
running	1
developing	2
update	4
hopes	1
deliver	3
following

more	1
abstraction	1
user's	1
possible
better	1
2x
more	1
flexibility	4
scalable	4
modes	1
operation	2
(standalone	1
namenodes	1
trackers	1
builtin	1
support)
if	1
knowing	1
demand	2
capability	1
wonders	1
me

this	1
oci0910812	1
testbed	1
afforded	1
entitled	1
exploring	1
frameworks	1
heavy	2
awarded	1
oci0910847

last	1
sunday	1
february	1
9	2
256	1
pmcontact	1
valid	2
xhtml	1
strict	2
css	1
hosting	1
saves	4
money
the	1
business	9
adopt	1
whether	3
offers	5
return	1
investment	1
what’s	1
great	3
don’t	1
hardware	4
hosted	1
provider’s	1
servers	3
savings	1
dispense	1
centre	1
won’t	1
pay	2
power	3
security	5
insurance	1
airconditioning	1
ongoing	1
maintenance

2	1
instantly	2
pricing
if	1
you’ll	1
buy	2
expensive	2
shortterm	1
firstly	1
you’re	1
lay	1
redundant	1
secondly	1
unexpectedly	1
deal	3
it

with	1
unexpected	1
peak	1
website	1
lasts	1
day	3
decreases	1
charged	1
upgrade	1
bigger	2
hire	1
increased	1
agility	1
costeffective	1
resourcing

3	1
strategic	1
competitive	1
advantage
deployment	1
virtually	1
success	1
online	1
almost	1
giving	2
advantage	2
competitors	1
who	3
adopted	1
technology

in	1
companies	7
normally	1
disadvantage	1
larger	1
greater	1
inhouse	1
themselves	2
playing	1
field	5
invest	1
heavily	1
centres

4	1
availability
as	1
businesses	6
reliant	1
downtime	2
disastrous	1
failure	3
coming	1
standstill	1
challenging	1
sometimes	2
longwinded	1
losses	2
significant

cloud	1
removes	1
possibility	1
causing	1
engineered	1
predictable	1
consistent	1
uptime	1
clients	1
issue	2
migrated	1
cloud’s	1
hyperconverged	1
design	1
guards	1
enabling	1
missioncritical	1
time

5	1
lightning	1
performance
in	1
stay	3
providers	6
continually	1
meet	2
demands	1
customers	3
powerful	3
super	1
ssd	1
drives

in	1
balancing	1
client	1
requests	1
maximises	1
utilisation	1
busy	1
suffers	1
–	2
disposal	1
provider	5
consistently	1
optimised

6	1
apps	2
quicker
before	1
advent	1
smoothly	1
cloudbased	2
significantly	1
reduced	1
successful	1
signup

this	1
advantages	3
migrate	1
benefitting	1
ai	2
signing	1
up

7	1
security
with	1
protected	2
hacking	1
infection	1
internal	2
theft	1
comply	1
stringent	1
regulations	1
protect	1
customers’	1
includes	1
robust	2
firewall	2
intrusion	1
prevention	1
inflow	1
virus	1
protection	2
isolate	1
threats	1
reach	2
extend	1
office	2
extensive	1
vpn	1
features

8	1
working
cloud	1
employees	2
internet	4
staff	1
workplace	1
cut	1
amount	3
councils	1
massively	1
expenditure	1
allowing	2
hotdesking	1
days	3
office

staff	1
company’s	3
webenabled	2
smartphones	2
laptops	1
collaborate	1
realtime	1
others	1
synchronised	1
conferencing	2
‘bring	1
(byod)’	1
policy	2
backed	2
logical	1
authentication	1
protocol	1
security

9	1
environmental	1
friendly
moving	1
carbon	1
footprint	1
keep	3
cool	1
lit

however	1
benefits	3
offloading	1
economies	1
energy	2
onsite	1
impact	1
environment

10	1
enabled
we’ve	1
puts	1
ones	2
collecting	1
processing	2
exponentially

big	1
analytics	4
vital	1
driving	1
carry	1
enormous	2
deployment	4
processing

conclusion
as	1
post	1
it’s	1
highly	1
reliable	1
systems

if	1
managed	2
247	1
expert	2
technical	1
packages	2
computing
cloud	1
anywhere	1
today's	2
tablets)	1
cloud

reduced	1
costs
moving	1
managing	3
purchasing	1
equipment	1
because

the	1
upgrades	1
contract
you	1
wages	1
staff
your	1
consumption	1
reduced
there	1
fewer	1
delays
scalability
your	1
suit	1
situation	1
frees	1
business

business	1
continuity
protecting	1
continuity	2
planning	1
experience	1
natural	1
disaster	1
crisis	1
secure	3
safe	2
conduct	1
usual	1
minimising	1
productivity

collaboration	1
efficiency
collaboration	1
ability	4
easily	4
locations	1
contractors	1
third	1
parties	1
model	4
records	1
advisers	1
accounting	1
accountant	1
financial	2
adviser)

flexibility	1
practices
cloud	1
practices	1
holiday	1
commute	1
(providing	1
connection)	1
offsite	1
connect	1
easily

access	1
updates
access	1
updates	2
requirements	1
fee	1
depending	1
regularly	2
include	1
power

also	1
consider
read	1
considerations
find	1
digital	2
strategy	3
business
improve	1
skills	3
advance	1
queensland	1
champions	1
honest	1
terrified	1
join	1
international	1

student	1
academically	1
nightmare	1

hear	1
say	1
felt	1
overwhelming	1

volume	1

academical	1
fit	3
ten	1
chapters	1
scare	1
did	1

start	1
milestone	1
dates	1
popular	3
concerned	1

topic	1

major	1
excessively	1
career	1

such	1
communication	2
pops	1

carries	1
tons	1
2020	1
mature	1
multicloud	1
likely	2
focused	2
vertical	1
sales	1
ground	1
war	1
leading	1
vendors	2
battle	1
market	3


picking	1
services	6
isn't	1
enterprise	1
boils	1
"it	1
depends"	1
web	3
azure	1
platform	6
infrastructure	6
dell	1
technologies	1
hewlettpackard	1
vmware	1
hybrid	4
deployments	1
ditto	1
likes	1
salesforce	1
adobe	1
workday	1
sap	1
oracle	4
databaseasaservice	1
player	1


that	1
trends	1
shifted	1
2019	1
2018	1
2017	1
level

the	1
covid19	1
pandemic	1
video	1
accelerating	2
moves	1
enterprises	1
increasingly	1
seeing	1
improves	1
forced	1
stayathome	1
orders	1
collaboration	1
tools	2
teams	2
became	1
cogs	1
companies'	1
broader	2

multicloud	1
selling	1
aspirational	1
goal	1
aware	1
vendor	2
lockin	1
clouds	1
theme	1
promoted	1
legacy	1
platforms	1
dose	1
(see	1
biggest	2
trend	1
goto	1
aws	2
grab	1
wallet	1
share)
the	1
game	1
acquisition	1
corporate	1
sticky	1
customer	2
secret	1
pitching	1
house	1
personalized	1
experiences	1

artificial	1
iot	2
edge	1
differentiators	1
serverless	1
gone	1
early	1
adds	1
rapid	1
clip	1
aws'	1
upsell	1
differentiate	1
gained	1
machinelearning	1
knowhow	1

sales	1
tactics	1
play	3
fear	1
uncertainty	1
doubt	1
norm	1
toward	1
surprisingly	1
reinvent	1
appeared	1
mindshare	1
press	1
sniped	1
industries	1
computing?	1
know)	1
management	1

in	1
recognize	1
automate	3
simplize	1
timely	2
accurate	1
convenient	1
applies	1
control	4
monitor	3
helpful	1
helps	2

depends	1
mostly	1
models	4
centralize	1
ondemand	1
position	2
administrator	3
customize	1
appropriate	1
goals	1
organization	3
paper	2
unforeseeable	1
usages	1
concern	1
affects	2
sustainability	1
company	4
usage	2
talked	1
aspect	1
including	3
present	1
holes	2
proposed	2
reinforcing	1
encryption	3
algorithm	1
forecast	2
catch	1
auditing	2
giants	2
jump	1
human	1
professional	1
engineer	1
viable	1
organization’s	1
board	2
bring	1
evitable	1
yottabyte	2
shortly	1
leverage	1
unit	2
gather	2
accuracy	1
efficiency	1
effectively	3
minimize	1
manual	1
orderly	2
configuring	1
arrange	1
tags	1
help	2
6	1
variety	3
compatible	1
os	2
developers	3
specialize	1

tran	5
46	1

7	1
controllers	1
administrators	1
immediate	1
verifyin	1
providing	1
authorization	1
verification	2
8	1
meets	1
secured	2
outage	1
transit	1
redo	1
patches	2
manually	2
break	1
interrupt	1
decides	1
performing	1
condition	1
replacement	1
regulation	1
accessing	1
twentyfirstcentury	1
obstacle	1
especially	1
privacy	1
concerns	1
hijacking	1
isolation	1
algorithms	1
encrypt	2
transfer	2
rsa	1
des	1
aes	1
tackle	1
hackers	1
anymore	1
message	1
asymmetric	1
cipher	2
sender	1
receiver	1
public	2
private	2
symmetric	1
cryptographic	1
decrypt	1
shift	1
combination	2
(symmetric)	1
(asymmetric)	1
technique	2
layers	2
14	1

the	1
validating	2
password	1
(otp)	1
people	3
authorized	1
generated	2
combining	1
techniques	1
ase	1
decryption	1
begins	1
(khana	1
et	1
al	1
288290)	1
•	2
significance	2
cryptography	2
complex	1
layer	2
implemented	1
sophisticated	1
strengthen	1
benefit	2
adaptable	1
noticeable	1
compared	1
target	1
inaccurate	1
depends	5
accessibility	1
search	1
anytime	1
globally	1
born	1
5g	1
lowers	1
latency	1
stable	1
roll	1
timeconsuming	1
employers	1
costly	2
onboard	1
care	2
enhance	1
prevent	1
steal	1
cybersecurity	1
warfare	1
stolen	1
send	2
spam	1
email	1
credibility	1
collaborations	1
connects	1
wifi	1
cellular	1
connected	1
groups	3
department	1
better	3
intensely	1
workload	2
routers	1
transferring	1
packet	1
identification	1
responsible	2
delivering	1
packets	1
destination	1
decrypted	1
method	3
focus	1
discuss	1
chapter	2
18	1
sending	1
places	1
crucial	1
musthave	1
controls	1
harm	1
sides	1
external	1
side	1
revalidate	1
vice	1
versa	1
cable	1
planned	1
cables	1
suitable	2
bandwidth	1
varying	1
keeps	1
sufficiently	1
difficult	1
fix	1
reasons	1
modem	1
router	1
transmitting	1
encoding	1
wave	1
component	1
drive	2
rotatingdisk	1
tape	1
incoming	1
lighter	1
consider	1
dependencies	1
compatibility	1
arm	1
leg	1
components	2
virtualization	1
comfortable	1
imagine	1
refers	1
situations	1
maybe	1
enables	2
test	1
differs	1
virtualization?	1
terms	1
confused	1
difference	2
delivers	1
delivered	1
(aas)	1
transferred	1
basically	1
empowers	1
works	2
products	1
combined	1
varied	1
cleared	1
hypervisor	1
virtualize	1
host	1
six	1
organizing	1
separating	1
decrease	1
complexity	2
manage	2
virtualizing	1
segregate	1
interconnected	1
archive	1
retrieving	1
beneficial	1
divide	1
virtualizes	1
identity	1
basic	2
simulated	1
approach	1
manipulate	1
requiring	1
indepth	1
actifio	1
denodo	1
integrator	1
21	1
desktop	1
workstation	1
convenience	2
drexel	1
college	1
informatics	1
(cci)	1
students	1
lab	1
class	1
window	1
encapsulate	1
moreover	2
users’	1
bluestacks	1
let	1
android	1
mac	1
(eugene)	1
ecommerce	1
tool	1
cli	1
shell	1
saving	1
countless	1
scripting	1
station	1
track	1
happening	1
workforce	1
organized	1
category	2
volume	2
arranged	1
strategizing	1
sql	2
jquery	1
ddl	1

cloud	1
technically	2
locally	1
unstructured	1
dramatically	1
inevitable	1
devoting	1
economical	1
prevents	1
corruption	1
velocity	2

to	1
ways	2
image	1
(dbaas)	2
provisioning	1
rules	1
main	1
databases	2
levels	1
lower	1
price	1
(shuff)	1
offer	1
paas	1
iaas	1
saas	1
functionality	1
furthermore	1
charge	1
optin	1
underlying	1

there	1
centralized	1
databased	1
relational	1
arranges	1
relationship	1
finding	1
breakdown	1
25	1

a	1
programmed	1
(structured	1
query	1
language)	1
noted	1
meanwhile	1
nosql	1
introduced	1
(anjomshoaa	1
tjoa	1
2)	1

so	1
imperative	1
distinguish	1
straightforward	1
vertically	1
horizontally	1
db2	1
mysql	1
nuodb	1
newly	1
rewrite	1
becomes	1
resolved	1
json	1
binary	1
capabilities	1
data?	1

as	1
stack	1
asymptote	1
held	1
world	1
selfdefined	1
definition	1
annually	1
analyze	1
integrity	1
cio	1
considers	1
merely	1
integration	1
besides	1
(bi)	1
decisionmaking	1
bi	1
misunderstanding	1
firm	1
scenario	1
acquire	1
consultants	1
identify	1
error	1
report	1
selfdatadriven	1
