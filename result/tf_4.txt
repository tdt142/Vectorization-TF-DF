writing	1
hadoop	56
applications	2
in	29
python	7
with	11
streaming
home
dataintensive	1
computing
hadoop
hadoop	1
streaming	9
python
1	1
introduction
one	1
of	48
the	105
unappetizing	1
aspects	1
to	47
users	1
traditional	3
hpc	3
is	32
that	19
it	13
written	2
java	3
not	6
designed	1
be	5
a	51
highperformance	2
language	2
and	30
although	1
i	8
can	15
only	5
definitively	1
speak	1
for	7
myself	1
suspect	1
learning	2
high	1
priority	1
domain	1
scientists	1
as	9
turns	1
out	2
though	2
allows	1
you	27
write	2
mapreduce	5
code	2
any	3
want	4
using	5
interface	3
this	21
key	10
feature	1
making	1
more	7
palatable	1
scientific	1
community	1
means	1
turning	1
an	3
existing	1
or	3
perl	2
script	7
into	8
job	23
does	2
require	1
derived	1
hadoopcentric	1
languages	2
like	2
pig	1
following	3
guide	3
illustrates	2
how	3
run	3
jobs	5
entirely	1
without	3
ever	1
having	2
mess	1
other	1
on	15
xsede's	1
gordon	5
resource	3
at	3
sdsc

once	1
basics	2
running	15
pythonbased	1
are	8
covered	1
will	13
illustrate	1
practical	1
example	5
parse	1
variant	1
call	1
format	1
(vcf)	1
file	6
vcf	2
parsing	2
library	2
would	3
install	1
root	1
privileges	1
your	9
supercomputing	1
account

the	1
wordcount	4
here	1
my	3
github	2
account	1
i've	2
also	3
got	1
implementations	1
r	1
posted	1
there

2	1
review	3
hpc
this	1
assumes	2
familiar	1
conceptual	2
level	2
should	4
good	1
shape	1
after	6
reading	1
overview	1
page	3
have

this	1
understand	1
cluster	8
(supercomputer)	1
mean

using	1
noninteractive	3
allencapsulated	1
setup	1
teardown	1
all	11
one	3
described	3
by	14
user	1
guide's	1
hadoop
creating	1
semipersistent	2
submitting	3
hand	1
clusters	1
supercomputers
for	1
purposes	1
keeping	1
process	3
specifically	1
clear	1
assume	1
we	11
have	4
already	1
spun	1
up	4
provide	1
relevant	1
commands	1
involved	1
if	7
go	2
route	1
submit	2
wraps	1
problem	3
single	2
job

3	1
canonical	2
example
counting	1
number	4
words	4
large	2
document	3
"hello	1
world"	1
among	1
simplest	1
full	2
map+reduce	1
recalling	1
map	10
step	6
transforms	2
raw	3
input	8
data	6
keyvalue	8
pairs	7
reduce	7
desired	1
output	6
conceptually	1
describe	1
act	1
counting	2
as

the	1
take	1
text	3
our	7
(i	1
use	3
herman	1
melville's	1
classic	1
moby	2
dick)	2
convert	1
each	3
word	2
keys	5
(words)	1
value	9
1
the	1
combine	1
duplicate	1
adding	2
their	1
values	1
since	1
every	1
(word)	1
has	2
1	3
list	2
unique	2
corresponding	1
key's	4
(word's)	1
count
schematic	1
context	1
mapreduce
with	1
need	1
program	2
acts	2
mapper	9
reducer	9
these	2
must	1
inputoutput	1
streams	1
such	2
way	2
equivalent	1
series	1
pipes

$	1
cat	1
inputtxt	1
|	6
mapperpy	3
sort	2
reducerpy	2
>	1
outputtxt
that	1
sends	3
via	2
stdin	1
then	2
mapped	1
stdin

31	1
mapper
the	1
above	2
quite	2
simple	1
implement	1
look	2
something	3
this

#usrbinenv	1
python

import	2
sys

for	1
line	5
sysstdin
	2
	226
=	10
linestrip()
	1
linesplit()
	1
keys
	1
1
	1
print(	3
"%s\t%d"	3
%	3
(key	1
value)	1
)
where

hadoop	1
from	4
("line"	1
being	3
defined	1
string	1
terminated	1
linefeed	1
character	1
\n)
python	1
strips	1
leadingtrailing	1
whitespace	2
(linestrip()
python	1
splits	1
individual	1
along	1
(linesplit())
for	1
(which	1
become	1
key)	1
assign	1
print	2
pair	1
separated	1
tab	1
(\t)
a	1
detailed	1
explanation	1
found	1
yahoo's	1
excellent	1
tutorial

32	1
shuffle
a	1
lot	1
happens	1
between	1
steps	1
largely	1
transparent	1
developer	1
brief	1
mappers	5
transformed	1
distributed	1
reducers	2
(termed	1
shuffle	1
step)	1
that

all	1
sorted	2
before	1
presented	2
function
all	1
sharing	1
same	4
sent	1
reducer
these	1
two	4
points	1
important	1
because

as	1
read	1
encounter	1
different	1
last	1
processed	1
know	2
previous	3
never	1
appear	1
again
if	1
gain	1
no	2
parallelization	1
come	1
happens
33	1
reducer
the	1
thus	1
loop	1
over	1
keypairs	1
apply	2
logic

if	1
key
	1
add	3
total
otherwise
	1
name	2
total
	1
reset	1
total	7
0
	1
and
	1
"this	1
key"	1
now	1
considered	1
"previous	1
key"
translating	1
little	3
extra	1
tighten	1
logic	1
get

#usrbinenv	1
sys

last_key	1
none
running_total	1
0

for	1
input_line	2
input_linestrip()
	1
this_key	1
input_linesplit("\t"	1
1)
	1
int(value)

	1
last_key	3
==	2
this_key
	2
running_total	2
+=	1
value
	2
else
	1
last_key
	1
(last_key	2
running_total)	2
)
	1
this_key

if	1
)
34	1
job
if	1
reducing	1
first	2
download	1
(moby	1
load	1
hdfs	3
purposely	1
am	1
renaming	1
copy	1
stored	1
mobydicktxt	1
instead	1
original	1
pg2701txt	3
highlight	1
location	2
file

$	1
wget	1
httpwwwgutenbergorgcacheepub2701pg2701txt
$	1
dfs	3
mkdir	1
wordcount
$	1
copyfromlocal	1
wordcountmobydicktxt
you	1
verify	1
was	2
loaded	2
properly

$	1
ls	1
wordcountmobydicktxt
found	1
items
rwrr	1
2	1
glock	1
supergroup	1
1257260	1
20130717	1
1324	1
userglockwordcountmobydicktxt
before	1
make	1
sure	1
scripts	2
actually	3
work	2
just	2
matter	2
them	2
through	2
pipes	1
bit	3
sample	1
(eg	1
1000	1
lines	1
dick)

$	1
head	1
n1000	1
reducerpy

young	1
4
your	1
16
yourself	1
3
zephyr	1
1
once	1
mapperreducer	1
errors	1
plug	1
accomplish	1
jar	3
hadoopstreamingxyzjar	1
comes	1
standard	1
apache	1
distribution	1
$hadoop_homecontribstreaming	1
where	1
$hadoop_home	1
base	1
directory	1
installation	1
xyz	1
version	1
opthadoopcontribstreaminghadoopstreaming103jar	3
so	2
actual	1
launch	1
command	1
like

$	1
\
	10
"python	4
$pwdmapperpy"	2
$pwdreducerpy"	2
"wordcountmobydicktxt"	1
"wordcountoutput"

packagejobjar	1
[scratchglock819550gordonfe2localhadoopglockdatahadoopunjar4721749961014550860	1
[	1
tmpstreamjob7385577774459124859jar	1
tmpdir=null
130717	1
192616	8
info	7
utilnativecodeloader	1
nativehadoop	1
library
130717	1
warn	1
snappyloadsnappy	1
snappy	1
native	1
loaded
130717	1
mapredfileinputformat	1
paths	1
1
130717	1
streamingstreamjob	5
getlocaldirs()	1
[scratchglock819550gordonfe2localhadoopglockdatamapredlocal
130717	1
job_201307171926_0001
130717	2
kill	2
run
130717	1
opthadooplibexecbinhadoop	1
dmapredjobtracker=gcn1334ibnet054311	1
tracking	1
url	2
httpgcn1334ibnet050030jobdetailsjsp?jobid=job_201307171926_0001
and	1
point	1
"tracking	1
url"	1
deceptive	1
probably	1
won't	1
able	1
access	1
fortunately	1
there	1
commandline	1
monitoring	1
somewhat	1
similar	1
qstat	1
noting	1
jobid	1
(highlighted	1
above)	1
do

$	1
status	2
job_201307171926_0001
job	1
job_201307171926_0001
file	1
hdfsgcn1334ibnet054310scratchglock819550gordonfe2localhadoopglockdatamapredstagingglockstagingjob_201307171926_0001jobxml
tracking	1
httpgcn1334ibnet050030jobdetailsjsp?jobid=job_201307171926_0001
map()	1
completion	2
10	1

reduce()	1
10

counters	1
30
	1
counters
	2
launched	4
tasks=1
	3
slots_millis_maps=16037
	1
time	5
spent	4
reduces	2
waiting	4
reserving	4
slots	4
(ms)=0
	4
maps	2
tasks=2
	1
datalocal	2
tasks=2

since	1
runs	2
foreground	1
another	1
terminal	1
(with	1
hadoop_conf_dir	1
properly	1
exported)	1
check	1
while	2
however	1
metrics	1
finished	1
highlighted	1
see	1
used	1
task	1
tasks	2
despite	1
than	2
nodes

35	1
adjusting	2
parallelism
unlike	1
parallelism	1
necessarily	2
size	1
compute	1
ultimately	1
determined	1
nature	1
due	1
distributes	1
chunks	2
"suggest"	1
when	1
doing	1
applying	1
change	1
highlighted

$	1
d	2
mapredmaptasks=4	1
wordcountmobydicktxt	1
wordcountoutput

$	1
job_201307172000_0001

	1
slots_millis_maps=24049	1

	1
racklocal	1
tasks=4
	1
tasks=3
similarly	1
mapredreducetasks=4	1
suggest	1
count	1
flexible	1
set	1
zero	1
file

with	1
said	1
fact	1
defaults	1
says	1
about	1
we're	1
trying	1
solvethat	1
entire	1
very	1
stupid	1
concepts	1
neatly	1
12	1
mb	2
waste	1
done	1
because	1
default	1
assigns	1
increments	1
64	1
meant	1
handle	1
multigigabyte	1
files	2
getting	1
do	2
useful	1
research	1
often	1
requires	1
knowledge	1
what	1
above

to	1
fill	1
gaps	1
next	1
part	1
tutorial	1
shows	1
applied	1
solve	1
realworld	1
involving	1
some	2
exotic	1
libraries	1
notcompletelyuniform	1
files

this	1
developed	1
support	1
national	1
science	1
foundation	1
(nsf)	1
under	1
grant	1
0910812	1
indiana	1
university	1
"futuregrid	1
experimental	1
grid	1
testbed"	1
opinions	1
findings	1
conclusions	1
recommendations	1
expressed	1
material	1
those	1
author(s)	1
reflect	1
views	1
nsf

1	1
introduction
2	1
hpc
3	1
example
31	1
mapper
32	1
shuffle
33	1
reducer
34	1
job
35	1
parallelism
last	1
modified	1
october	1
20	1
2019
glenn@glennklockwoodcom	1
