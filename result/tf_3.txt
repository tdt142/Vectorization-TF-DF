conceptual	2
overview	3
of	76
mapreduce	26
and	45
hadoop
home
dataintensive	1
computing
hadoop
overview	1
hadoop
1	1
introduction
this	1
page	1
serves	1
as	12
a	58
30000foot	1
the	156
programming	1
paradigm	3
key	16
features	1
that	30
make	2
it	20
useful	1
for	15
solving	2
certain	2
types	1
computing	10
workloads	1
simply	1
cannot	1
be	19
treated	1
using	6
traditional	11
parallel	21
methods	2
only	4
covers	2
broad	1
basics	1
is	50
intended	1
to	78
provide	1
you	12
with	12
information	1
need	2
follow	2
subsequent	2
pages	1
i've	1
written	2
on

writing	1
hadoop	15
applications	4
in	32
python	3
streaming
parsing	1
vcf	1
files	8
streaming
parallel	1
r	1
hadoop
2	1
comparing	2
parallelism
in	1
order	2
appreciate	1
what	5
brings	5
table	1
i	4
think	1
most	3
meaningful	2
contrast	3
call	1
problems	8
define	1
"traditional"	1
those	4
which	10
use	3
libraries	1
like	6
mpi	2
openmp	1
cuda	1
or	10
pthreads	1
produce	1
results	4
by	12
utilizing	1
multiple	7
cpus	1
perform	3
some	6
sort	4
numerical	2
calculation	3
concurrently	1
are	24
well	1
suited	2
being	4
solved	1
these	9
typically	2
share	2
two	2
common	5
features

they	1
cpubound	1
part	3
problem	3
takes	4
time	2
doing	2
calculations	5
involving	1
floating	1
point	1
integer	1
arithmetic
input	1
data	51
gigabytescale	2
necessary	3
describe	1
conditions	1
less	1
than	2
hundred	2
gigabytes	1
very	3
often	2
few	1
megabytes	1
at	4
most
item	1
#1	1
may	4
seem	1
trivial	1
after	2
all	19
computers	1
meant	1
compute	23
so	4
wouldn't	1
parallelized	1
fundamentally	1
limited	1
how	3
quickly	1
computer	2
can	14
do	3
calculations?

traditionally	1
answer	1
this	12
question	1
has	3
been	2
yes	1
but	4
technological	1
landscape	1
rapidly	2
changing	3
over	3
last	1
decade	1
sources	1
vast	1
unending	1
(eg	4
social	1
media	1
inexpensive	2
genenome	1
sequencing)	1
have	7
converged	1
highcapacity	2
hard	1
drives	1
advanced	1
filesystems	1
support	1
them	1
now	3
dataintensive	4
emerging	1
aforementioned	1
demonstrate	2
following	2
features

input	1
far	2
beyond	1
datasets	2
commonly	1
on	33
tens	1
hundreds	1
thousands	1
terabytes
they	1
iobound	1
longer	1
get	2
from	8
its	2
permanent	2
location	1
cpu	2
operate	1
data
21	1
applications
to	1
illustrate	1
differences	1
schematic	1
depicts	2
your	14
typical	2
traditionally	3
application	1
works

program	1
flow	4
problem
the	1
input	20
stored	5
remote	1
storage	5
device	3
(a	1
san	1
file	10
server	1
serving	1
nfs	1
lustre	1
gpfs	1
filesystem	8
etc	1
grey	1
cylinders)	1
resources	4
elements	7
(blue	2
boxes)	1
abstract	4
units	1
represent	1
ranks	1
nodes	8
threads	1
sharedmemory	1
system

upon	1
launching	1
application

a	1
master	2
worker	3
(mpi	1
rank	1
thread	1
etc)	1
reads	1
disk	2
(green	2
arrow)
the	1
then	10
divides	1
up	4
into	12
chunks	5
sends	1
parts	1
each	12
other	11
workers	6
(red	1
arrows)
all	1
their	11
chunk	5
data
all	1
communicate	2
continue	2
next	3
iteration	1
calculation
note	1
cases	1
io	2
api	1
mpiio	2
collectively	1
read	2
resides	2
must	3
highperformance	1
sustain	1
required	1
networkread	1
bandwidth

the	1
fundamental	2
limit	1
scalability	1
here	2
step	9
#1the	1
process	2
reading	1
arrow)	2
performed	1
serially	1
even	1
if	4
ingestion	1
separated	1
squares)	1
pipe	1
through	5
finite	1
rate	1
while	3
possible	1
increase	2
speed	1
connection	1
between	1
throwing	1
more	4
money	1
buying	1
fast	1
ssds	1
faster	1
networking	1
andor	1
servers)	1
cost	1
does	4
not	6
scale	1
linearly

22	1
applications
the	1
completely	2
different	1
way	2
subset	1
parallelizable	1
gets	2
around	1
bottleneck	2
ingesting	1
(that	1
pesky	1
green	1
whereas	1
parallelism	3
oppositeit	1
data

program	1
problem
in	1
separate	2
system	2
rather	1
exists	5
little	2
pieces	2
permanently	1
allows	1
our	1
procedure	1
steps

we	1
don't	1
move	3
any	6
since	3
predivided	1
already	5
capable	1
acting	1
elements
all	1
functions	1
sent	2
where	6
respective	1
exist	2
calculations
all	1
calculation
thus	1
needs	1
moved	1
when	2
communicating	1
#3	1
there	1
no	1
serial	1
loaded	1
before	4
distributed	8
because	2
resources

of	1
course	1
able	1
independent	2
principal	1
constraint	1
jobs	4
ideally	1
trivially	2
large	4
quantities	1
worker's	1
depend	1
will	6
begin	1
encounter	1
diminishing	1
returns

3	1
	2
implementation
now	1
we've	1
established	1
description	1
concept	2
bringing	1
we	1
equipped	1
look	1
an	9
actual	5
implementation	2
mapreduce

31	1
magic	4
hdfs
the	1
idea	1
underpinning	1
mapreducebringing	1
instead	1
oppositeshould	1
sound	2
simple	1
solution	1
inherent	1
however	2
devil	1
details	1
implementing	1
framework	2
single	8
transparently	2
diced	1
across	5
physical	3
(all	1
appearing	1
remain	1
user)	1
trivial

hadoop	1
perhaps	1
widely	1
used	2
accomplishes	2
feat	1
hdfs	13
provides	2
chunking	1
distribution	2
efficient	1
we're	1
talking	1
about	1
let's	1
refer	1
nodes

hdfs	1
copy	2
manner	1
unlike	1
many	1
commands	1
manipulating	1
(ls	1
mkdir	1
rm	1
mv	1
cp	1
cat	1
tail	1
chmod	1
name	2
few)	1
behave	1
might	3
expect	1
standard	1
linux's	1
ext4)

the	1
magical	1
going	1
just	4
underneath	1
surface	1
although	2
appears	2
contains	1
reality	1
nodes

schematic	1
depicting	1
hdfs
when	1
depicted	1
above	2
sliced	1
64	4
mb	5
"chunks"	1
replicated	3
three	3
times	3
reliability	1
various	1
cluster	1
given	4
physically	2
chunked	1
triplicate	1
interactions	1
still	2
appear	1
same	10
copied	1
initially	1
thus	1
handles	1
burden	1
slicing	1
distributing	2
recombining	1
you

hdfs's	1
size	3
replication
the	1
(block)	1
choice	1
replicate	1
hdfs's	1
default	3
values	6
decisions	1
changed

the	1
block	1
modified	3
dfsblocksize	1
property	3
hdfssitexml	2
128	1
production	1
environments
the	1
replication	2
factor	1
dfsreplication	1
also	1
changed	1
perfile	1
basis	1
specifying	1
d	1
dfsreplication=1	1
put	1
command	2
line	2
dfs	2
setrep	1
w	1
1	1
command
32	1
jobs
hdfs	1
interesting	1
technology	1
automatic	1
recovery	1
userspace	1
relatively	1
easy	2
configure	1
conceptually	1
understand	1
true	1
utility	1
comes	1
light	1
executed	1
hdfs

as	1
implies	1
principally	1
comprised	1
steps	1
map	5
reduce	5
overall	1
workflow	1
generally	1
looks	1
something	1
this

program	1
application
the	1
left	1
half	1
diagram	1
described	1
previous	1
section	1
copyfromlocal	1
automatically	1
moving	2
strictly	1
job	4
(ie	1
dataset	1
home	1
would	1
filesystem)	1
job's	2
started

321	1
step
once	2
initiated	1
step

launches	1
number	3
mappers	4
contain	1
data
for	1
mapper	5
"splits"	1
individual	1
lines	1
text	3
newline	2
characters	2
(\n)
each	1
split	3
(line	1
was	1
terminated	1
\n)	1
function
your	1
function	4
expected	1
turn	1
zero	1
keyvalue	17
pairs	16
"emit"	1
step
that	1
step's	1
transform	1
raw	2
series	1
expectation	1
parsed	1
analyzed	1
meaningfully	1
it's	1
perfectly	1
fine	1
duplicate	1
keys	6
emitted	2
mappers

the	1
decision	1
along	1
behavior	1
assumes	1
ascii	1
change	1
passed	3
alternate	1
inputformats

322	1
finished	1
digesting	1
sorted	1
according	1
reducers	8
such	1
sharing	4
always	2
go	2
reducer	10
corollary	1
one	2
particular	1
specific	1
guaranteed	1
continuous	2
strip	1
received

your	1
based	1
example	2
calculate	1
sum	1
word	2
count	1
example)	1
emit	2
back	1
unique	2
keys'	1
result	1
function's	1
calculation

the	1
shuffle
the	1
sorting	1
mapper's	1
output	2
seen	1
called	1
"shuffle"	1
really	1
happens	1
partitioner	3
determine	2
they	1
to

the	1
hashes	1
modulus	1
hash	2
pair	1
value	1
therefore	1
wind	1
reducer

once	1
assigned	1
loop	1
reducer's	1
examine	2
see	1
tutorial	3
writing	2
follows	1
essential	1
streaming	2
interface

this	1
complicated	1
without	2
sample	1
code	2
easier	1
working	1
example

4	1
summary
this	1
admittedly	1
dry	1
accompany	1
points	1
should	1
take	1
away

mapreduce	1
resources
hadoop	1
storing	1
fashion	1
hdfs
hdfs	1
stores	1
nodes
hdfs	1
presents	1
users	1
despite	1
fact
mapreduce	1
ideal	1
operating	1
flat	1
(unstructured)	1
operations	1
them
hadoop	1
stage	2
where
the	1
transforms	2
occur
the	1
value
if	1
interest	1
remaining	1
having	1
strongly	1
recommend	1
looking	1
my	1
much	1
material	1
context	1
(counting	1
text)	1
python

1	1
introduction
2	1
parallelism
21	1
applications
22	1
applications
3	1
implementation
31	1
hdfs
32	1
jobs
4	1
summary
last	1
october	1
20	1
2019
glenn@glennklockwoodcom	1
